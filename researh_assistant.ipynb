{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf0oNg4qXBQT"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch arxiv sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import arxiv\n",
        "import re\n",
        "from typing import Dict, List, Optional\n",
        "from enum import Enum\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "WmRNjKccXaZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentType(Enum):\n",
        "    ANALYSIS = \"analysis_agent\"\n",
        "    SEARCH = \"search_agent\"\n",
        "    GENERATION = \"generation_agent\"\n",
        "    INTERACTIVE = \"interactive_agent\""
      ],
      "metadata": {
        "id": "uZik9RlyXa-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HandoffDecision:\n",
        "    \"\"\"Structure\"\"\"\n",
        "    def __init__(self, target_agent: AgentType, reason: str, data: Dict):\n",
        "        self.target_agent = target_agent\n",
        "        self.reason = reason\n",
        "        self.data = data"
      ],
      "metadata": {
        "id": "J_waUK5sXc3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAgent:\n",
        "    \"\"\"A base class with a handover tool\"\"\"\n",
        "\n",
        "    def __init__(self, agent_type: AgentType):\n",
        "        self.agent_type = agent_type\n",
        "        self.handoff_registry = {}\n",
        "\n",
        "    def register_handoff(self, agent_type: AgentType, agent_instance):\n",
        "        \"\"\"Registers another agent for handoff\"\"\"\n",
        "        self.handoff_registry[agent_type] = agent_instance\n",
        "        print(f\"\\n {self.agent_type.value} → Registered handoff to {agent_type.value}\")\n",
        "\n",
        "    def handoff_to(self, target: AgentType, data: Dict, reason: str = \"\") -> Dict:\n",
        "        \"\"\"Handover tool - transfers work to another agent\"\"\"\n",
        "        print(f\"\\nHandoff: {self.agent_type.value} → {target.value}\")\n",
        "        print(f\"Reason: {reason}\")\n",
        "\n",
        "        # checking if the agent is registered(error handling)\n",
        "        if target not in self.handoff_registry:\n",
        "            print(f\"Error: Agent {target.value} isn't registered\")\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": f\"Agent {target.value} isn't registered\",\n",
        "                \"agent\": self.agent_type.value\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            target_agent = self.handoff_registry[target]\n",
        "            return target_agent.receive_handoff(self.agent_type, data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during handoff: {str(e)}\")\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": f\"Error during handoff: {str(e)}\",\n",
        "                \"agent\": self.agent_type.value\n",
        "            }\n",
        "\n",
        "    def receive_handoff(self, from_agent: AgentType, data: Dict) -> Dict:\n",
        "        \"\"\"Receiving a handover from another agent\"\"\"\n",
        "        print(f\"\\n {self.agent_type.value} receives work from {from_agent.value}\")\n",
        "        return {\"status\": \"received\", \"data\": data}\n",
        "\n",
        "    def decide_next_agent(self, current_data: Dict) -> Optional[HandoffDecision]:\n",
        "        \"\"\"Decides whether to handover and to whom\"\"\"\n",
        "        return None"
      ],
      "metadata": {
        "id": "fu_BUbNyXe43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent 1 - input analysis\n",
        "class RequestAnalysisAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(AgentType.ANALYSIS)\n",
        "        print(\"Loading analysis agent\")\n",
        "\n",
        "        try:\n",
        "            model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            print(\"Model loaded\")\n",
        "        except Exception as e:\n",
        "            self.model = None\n",
        "            print(f\"Model isn't available: {str(e)}\")\n",
        "            print(\"Using a rule-based approach\")\n",
        "\n",
        "    def analyze_and_handoff(self, user_input: str) -> Dict:\n",
        "        \"\"\"Main method - analyzing and automatically switching to search\"\"\"\n",
        "        print(f\"\\n\")\n",
        "        print(f\"Analysis agent is processing the request\")\n",
        "\n",
        "        #input validation(error handling)\n",
        "        if not user_input or not user_input.strip():\n",
        "            print(\"Error: The entry is empty\")\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": \"Entry cannot be empty\",\n",
        "                \"agent\": self.agent_type.value\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            analysis_result = self._analyze(user_input)\n",
        "\n",
        "\n",
        "            if analysis_result.get(\"status\") == \"error\":\n",
        "                return analysis_result\n",
        "\n",
        "            next_step = self.decide_next_agent(analysis_result)\n",
        "\n",
        "            if next_step:\n",
        "                return self.handoff_to(\n",
        "                    next_step.target_agent,\n",
        "                    next_step.data,\n",
        "                    next_step.reason\n",
        "                )\n",
        "\n",
        "            return analysis_result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error during analysis: {str(e)}\")\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": \"Request parsing error\",\n",
        "                \"agent\": self.agent_type.value\n",
        "            }\n",
        "\n",
        "    def decide_next_agent(self, current_data: Dict) -> Optional[HandoffDecision]:\n",
        "        \"\"\"Dynamic decision - where next?\"\"\"\n",
        "        if current_data.get(\"status\") == \"success\" and current_data.get(\"keywords\"):\n",
        "            return HandoffDecision(\n",
        "                target_agent=AgentType.SEARCH,\n",
        "                reason=\"Keywords found, literature search required\",\n",
        "                data=current_data\n",
        "            )\n",
        "        return None\n",
        "\n",
        "    def _analyze(self, user_input: str) -> Dict:\n",
        "        \"\"\"Internal method for analysis\"\"\"\n",
        "\n",
        "        user_input_clean = user_input.strip()\n",
        "\n",
        "        if len(user_input_clean) < 2:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": \"Query too short. Please provide at least 2 characters.\",\n",
        "                \"agent\": self.agent_type.value\n",
        "            }\n",
        "\n",
        "        words = re.findall(r'\\w+', user_input_clean.lower())\n",
        "\n",
        "        keywords = [w for w in words if len(w) >= 2][:7]\n",
        "\n",
        "        if not keywords:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": \"No valid keywords found. Please provide more specific search terms.\",\n",
        "                \"agent\": self.agent_type.value\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"main_topic\": user_input_clean[:80] if len(user_input_clean) > 80 else user_input_clean,\n",
        "            \"keywords\": keywords,\n",
        "            \"status\": \"success\",\n",
        "            \"agent\": self.agent_type.value\n",
        "        }"
      ],
      "metadata": {
        "id": "4Izf9Qz_XhcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent 2 - search literature\n",
        "class LiteratureSearchAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(AgentType.SEARCH)\n",
        "        print(\"Search agent ready\")\n",
        "\n",
        "    def receive_handoff(self, from_agent: AgentType, data: Dict) -> Dict:\n",
        "        \"\"\"Recieves the job and performs the search automatically\"\"\"\n",
        "        print(f\"Search agent recieved work from {from_agent.value}\")\n",
        "\n",
        "        try:\n",
        "            keywords = data.get(\"keywords\", [])\n",
        "\n",
        "            # no keywords(error handling)\n",
        "            if not keywords:\n",
        "                print(\"No keywords, trying with main_topic\")\n",
        "                main_topic = data.get(\"main_topic\", \"\")\n",
        "                if main_topic:\n",
        "                    keywords = [main_topic]\n",
        "                else:\n",
        "                    print(\"No keywords and no main_topic\")\n",
        "                    #interaction with user\n",
        "                    return self.handoff_to(\n",
        "                        AgentType.INTERACTIVE,\n",
        "                        {**data, \"error\": \"no_keywords\"},\n",
        "                        \"No keywords, I'm looking for user help\"\n",
        "                    )\n",
        "\n",
        "            #searching\n",
        "            search_result = self._search_arxiv(keywords)\n",
        "\n",
        "            #search error\n",
        "            if \"error\" in search_result:\n",
        "                error_type = search_result.get(\"error\")\n",
        "                print(f\"Search error: {error_type}\")\n",
        "\n",
        "                #timeout error\n",
        "                if error_type == \"timeout\" and len(keywords) > 1:\n",
        "                    print(\"I'm trying with fewer keywords...\")\n",
        "                    retry_result = self._search_arxiv(keywords[:2], max_results=3)\n",
        "                    if retry_result.get(\"papers_count\", 0) > 0:\n",
        "                        search_result = retry_result\n",
        "                        print(\"Retry succeded!\")\n",
        "\n",
        "                #network error\n",
        "                elif error_type == \"network\":\n",
        "                    print(\"Network error, continuing without results\")\n",
        "                    search_result = {\"papers\": [], \"papers_count\": 0}\n",
        "\n",
        "            #no results\n",
        "            papers_count = search_result.get(\"papers_count\", 0)\n",
        "            if papers_count == 0:\n",
        "                print(\"No results available\")\n",
        "\n",
        "                #broader search\n",
        "                if len(keywords) > 2:\n",
        "                    print(\"Trying to expand the search...\")\n",
        "                    broader_search = self._search_arxiv(keywords[:1], max_results=10)\n",
        "                    if broader_search.get(\"papers_count\", 0) > 0:\n",
        "                        search_result = broader_search\n",
        "                        print(\"Broader search succeded\")\n",
        "                    else:\n",
        "\n",
        "                        return self.handoff_to(\n",
        "                            AgentType.INTERACTIVE,\n",
        "                            {**data, \"search_results\": search_result, \"no_results\": True},\n",
        "                            \"No results, looking for alternative terms\"\n",
        "                        )\n",
        "\n",
        "            # combining results\n",
        "            combined_data = {**data, \"search_results\": search_result}\n",
        "\n",
        "            #hadnoff decision\n",
        "            next_step = self.decide_next_agent(combined_data)\n",
        "\n",
        "            if next_step:\n",
        "                return self.handoff_to(\n",
        "                    next_step.target_agent,\n",
        "                    next_step.data,\n",
        "                    next_step.reason\n",
        "                )\n",
        "\n",
        "            return combined_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Critical error in the search agent: {str(e)}\")\n",
        "            #fallback\n",
        "            return self.handoff_to(\n",
        "                AgentType.GENERATION,\n",
        "                {**data, \"search_results\": {\"papers\": [], \"papers_count\": 0}},\n",
        "                \"Critical error, continuing with no results\"\n",
        "            )\n",
        "\n",
        "    def decide_next_agent(self, current_data: Dict) -> Optional[HandoffDecision]:\n",
        "        \"\"\"Deciding what to do next after the search\"\"\"\n",
        "        papers_count = current_data.get(\"search_results\", {}).get(\"papers_count\", 0)\n",
        "\n",
        "        return HandoffDecision(\n",
        "            target_agent=AgentType.GENERATION,\n",
        "            reason=f\"\\n {papers_count} papers found, transfering to writing\",\n",
        "            data=current_data\n",
        "        )\n",
        "\n",
        "    def _search_arxiv(self, keywords: List[str], max_results: int = 5) -> Dict:\n",
        "        \"\"\"Internal search\"\"\"\n",
        "\n",
        "\n",
        "        if not keywords:\n",
        "            return {\"papers\": [], \"papers_count\": 0, \"error\": \"no_keywords\"}\n",
        "\n",
        "\n",
        "        if not isinstance(keywords, list):\n",
        "            print(f\"Keywords isn't a list, converting: {type(keywords)}\")\n",
        "            if isinstance(keywords, str):\n",
        "                keywords = [keywords]\n",
        "            else:\n",
        "                return {\"papers\": [], \"papers_count\": 0, \"error\": \"invalid_type\"}\n",
        "\n",
        "        try:\n",
        "            query = \" \".join(keywords[:3])\n",
        "            print(f\"Searching arXiv: {query}\")\n",
        "\n",
        "            search = arxiv.Search(\n",
        "                query=query,\n",
        "                max_results=max_results,\n",
        "                sort_by=arxiv.SortCriterion.Relevance\n",
        "            )\n",
        "\n",
        "            papers = []\n",
        "            timeout_counter = 0\n",
        "\n",
        "            for result in search.results():\n",
        "                timeout_counter += 1\n",
        "                if timeout_counter > max_results + 5:\n",
        "                    print(\"Search timeout\")\n",
        "                    return {\"papers\": papers, \"papers_count\": len(papers), \"error\": \"timeout\"}\n",
        "\n",
        "                papers.append({\n",
        "                    \"title\": result.title,\n",
        "                    \"authors\": [a.name for a in result.authors][:3],\n",
        "                    \"year\": result.published.year,\n",
        "                    \"summary\": result.summary[:400],\n",
        "                    \"url\": result.entry_id\n",
        "                })\n",
        "\n",
        "            print(f\"\\n {len(papers)} papers found\")\n",
        "            return {\"papers\": papers, \"papers_count\": len(papers)}\n",
        "\n",
        "        except TimeoutError:\n",
        "            print(\"arXiv API timeout\")\n",
        "            return {\"papers\": [], \"papers_count\": 0, \"error\": \"timeout\"}\n",
        "\n",
        "        except ConnectionError:\n",
        "            print(\"Network error\")\n",
        "            return {\"papers\": [], \"papers_count\": 0, \"error\": \"network\"}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Search error: {str(e)}\")\n",
        "            return {\"papers\": [], \"papers_count\": 0, \"error\": \"unexpected\"}"
      ],
      "metadata": {
        "id": "fzxOl9GVXnH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent 3 - text generation\n",
        "class TextGenerationAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(AgentType.GENERATION)\n",
        "        print(\"Loading Text Generation Agent\")\n",
        "\n",
        "        try:\n",
        "            model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            print(\"Generation agent loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.model = None\n",
        "            print(f\"Generation agent failed to load: {e}\")\n",
        "\n",
        "    def receive_handoff(self, from_agent: AgentType, data: Dict) -> Dict:\n",
        "        \"\"\"Receives search results and synthesizes them into a report.\"\"\"\n",
        "        print(f\"Writing agent is synthesizing results from {from_agent.value}...\")\n",
        "\n",
        "        try:\n",
        "            if \"main_topic\" not in data:\n",
        "                return {\"status\": \"error\", \"message\": \"Missing main_topic\"}\n",
        "\n",
        "\n",
        "            generated = self._generate_text(data)\n",
        "\n",
        "            final_data = {**data, \"generated_text\": generated}\n",
        "            return self.handoff_to(\n",
        "                self.decide_next_agent(final_data).target_agent,\n",
        "                final_data,\n",
        "                \"Synthesis complete.\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Generating error: {str(e)}\")\n",
        "            return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "    def _generate_text(self, data: Dict) -> str:\n",
        "        \"\"\"Constructs a prompt with research data and calls the LLM.\"\"\"\n",
        "        topic = data.get(\"main_topic\", \"Research\")\n",
        "        papers = data.get(\"search_results\", {}).get(\"papers\", [])\n",
        "\n",
        "        #prepare the context from search results\n",
        "        context_block = \"\"\n",
        "        for i, p in enumerate(papers, 1):\n",
        "            context_block += f\"Paper {i}: {p['title']} ({p['year']})\\nSummary: {p['summary']}\\n\\n\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert scientific writer. Write a professional literature review about: {topic}.\n",
        "\n",
        "        I will provide you with summaries of relevant research papers.\n",
        "        Use them to write:\n",
        "        1. An Introduction.\n",
        "        2. A Synthesized Literature Review (grouping ideas, not just listing papers).\n",
        "        3. A Conclusion on current trends.\n",
        "\n",
        "        RESEARCH DATA:\n",
        "        {context_block if context_block else \"No specific papers found. Provide a general overview.\"}\n",
        "\n",
        "        FORMATTING:\n",
        "        - Use Markdown headers (##).\n",
        "        - Use bold text for key concepts.\n",
        "        - Cite papers as [Paper 1], [Paper 2], etc.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.model:\n",
        "            return self._call_llm(prompt)\n",
        "        return \"Model not available. Manual fallback: \" + context_block\n",
        "\n",
        "    def _call_llm(self, prompt: str) -> str:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a professional academic writer.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        generated_ids = self.model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=1024,\n",
        "            do_sample=True,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    def decide_next_agent(self, current_data: Dict) -> HandoffDecision:\n",
        "        return HandoffDecision(\n",
        "            target_agent=AgentType.INTERACTIVE,\n",
        "            reason=\"Report generated.\",\n",
        "            data=current_data\n",
        "        )"
      ],
      "metadata": {
        "id": "j9weor62YJta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent 4 - interactive\n",
        "class InteractiveAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(AgentType.INTERACTIVE)\n",
        "        print(\"Loading Interactive Agent\")\n",
        "\n",
        "        try:\n",
        "            model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            print(\"Interactive agent loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.model = None\n",
        "            print(f\"Interactive agent failed to load: {e}\")\n",
        "\n",
        "    def receive_handoff(self, from_agent: AgentType, data: Dict) -> Dict:\n",
        "        \"\"\"Receives the final result and presents it to the user.\"\"\"\n",
        "        print(f\"\\n[System]: Interaction phase started.\")\n",
        "\n",
        "        if \"error\" in data or \"no_results\" in data:\n",
        "            self._handle_error_interaction(data)\n",
        "\n",
        "        if \"generated_text\" in data:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"GENERATED RESEARCH SUMMARY\")\n",
        "            print(\"=\"*50)\n",
        "            print(data[\"generated_text\"])\n",
        "            print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "        return {\n",
        "            **data,\n",
        "            \"status\": \"awaiting_interaction\",\n",
        "            \"message\": \"I have analyzed the research. You can ask me questions about it or ask for more details.\"\n",
        "        }\n",
        "\n",
        "    def handle_question(self, question: str, current_data: Dict) -> Dict:\n",
        "        \"\"\"Uses LLM to answer questions or decide if a new search is needed.\"\"\"\n",
        "        print(f\"\\nAgent is thinking...\")\n",
        "\n",
        "        context = current_data.get(\"generated_text\", \"No context available.\")\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a research assistant. Based on the context below, answer the user's question.\n",
        "\n",
        "        CONTEXT:\n",
        "        {context[:1500]}\n",
        "\n",
        "        USER QUESTION:\n",
        "        {question}\n",
        "\n",
        "        INSTRUCTION:\n",
        "        1. If the user wants 'more' information, 'new' papers, or topics NOT in the context, start your response with the word 'SEARCH_REQUIRED'.\n",
        "        2. Otherwise, answer the question naturally based on the context.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.model:\n",
        "            response_text = self._call_llm(prompt)\n",
        "        else:\n",
        "            response_text = \"Model not loaded. I cannot process this request.\"\n",
        "\n",
        "        if \"SEARCH_REQUIRED\" in response_text:\n",
        "            print(\"LLM decided a new search is necessary.\")\n",
        "            clean_response = response_text.replace(\"SEARCH_REQUIRED\", \"\").strip()\n",
        "            print(f\"Reason: {clean_response}\")\n",
        "\n",
        "            new_keywords = self._extract_keywords(question)\n",
        "\n",
        "            return self.handoff_to(\n",
        "                AgentType.SEARCH,\n",
        "                {**current_data, \"keywords\": new_keywords},\n",
        "                f\"User requested more info: {question}\"\n",
        "            )\n",
        "\n",
        "        print(f\"\\n[Agent]: {response_text}\")\n",
        "        return {\"message\": response_text, \"status\": \"clarification\"}\n",
        "\n",
        "    def _call_llm(self, prompt: str) -> str:\n",
        "        \"\"\"Helper to generate text from the LLM.\"\"\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful research assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        generated_ids = self.model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=True,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "\n",
        "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    def _handle_error_interaction(self, data: Dict):\n",
        "        \"\"\"Displays formatted error messages.\"\"\"\n",
        "        if \"no_results\" in data:\n",
        "            print(\"\\n[!] Warning: No specific papers were found for those keywords.\")\n",
        "        elif data.get(\"error\") == \"no_keywords\":\n",
        "            print(\"\\n[!] Error: I couldn't identify specific topics to search for.\")\n",
        "\n",
        "    def _extract_keywords(self, text: str) -> List[str]:\n",
        "        \"\"\"Simple keyword extraction for the search handoff.\"\"\"\n",
        "        words = re.findall(r'\\w+', text.lower())\n",
        "\n",
        "        stop_words = {'the', 'about', 'more', 'find', 'search', 'research', 'show', 'tell'}\n",
        "        return [w for w in words if len(w) > 3 and w not in stop_words][:5]"
      ],
      "metadata": {
        "id": "IAz9MO-jYRMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResearchOrchestrator:\n",
        "    \"\"\"An orchestrator that connects agents\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"Literature review system\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "        #agent creation\n",
        "        self.analysis_agent = RequestAnalysisAgent()\n",
        "        self.search_agent = LiteratureSearchAgent()\n",
        "        self.generation_agent = TextGenerationAgent()\n",
        "        self.interactive_agent = InteractiveAgent()\n",
        "\n",
        "        #handoff network\n",
        "        self._setup_handoffs()\n",
        "\n",
        "        print(\"\\nThe system is ready\\n\")\n",
        "\n",
        "    def _setup_handoffs(self):\n",
        "        \"\"\"Handover network configuration\"\"\"\n",
        "        self.analysis_agent.register_handoff(AgentType.SEARCH, self.search_agent)\n",
        "        self.search_agent.register_handoff(AgentType.GENERATION, self.generation_agent)\n",
        "        self.search_agent.register_handoff(AgentType.INTERACTIVE, self.interactive_agent)\n",
        "        self.generation_agent.register_handoff(AgentType.INTERACTIVE, self.interactive_agent)\n",
        "        self.interactive_agent.register_handoff(AgentType.SEARCH, self.search_agent)\n",
        "\n",
        "    def process_request(self, user_input: str) -> Dict:\n",
        "        \"\"\"Starts a chain of handovers\"\"\"\n",
        "        print(\"\\n\")\n",
        "        print(f\"New request: {user_input}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        try:\n",
        "            result = self.analysis_agent.analyze_and_handoff(user_input)\n",
        "\n",
        "            #check results\n",
        "            if result.get(\"status\") == \"error\":\n",
        "                print(f\" The system returned an error.: {result.get('message')}\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Critical error: {str(e)}\")\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": \"Critical system error\"\n",
        "            }\n",
        "\n",
        "    def ask_followup(self, question: str, previous_result: Dict) -> Dict:\n",
        "        \"\"\"Asks a follow-up question.\"\"\"\n",
        "        try:\n",
        "            return self.interactive_agent.handle_question(question, previous_result)\n",
        "        except Exception as e:\n",
        "            print(f\" Error while processing a question: {str(e)}\")\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": \"Error while processing a question\"\n",
        "            }"
      ],
      "metadata": {
        "id": "cPmHwf0PYXjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    system = ResearchOrchestrator()\n",
        "except Exception as e:\n",
        "    print(f\"Critical error: The system could not be initialized: {str(e)}\")\n",
        "    exit(1)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Interactive mode:\")\n",
        "print(\"\\n\")\n",
        "print(\"\\nHandovers will be shown in real time.\")\n",
        "print(\"For exit: 'exit'\\n\")\n",
        "\n",
        "current_result = None\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"\\nYour input: \").strip()\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"\\nSystem finished\")\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        if current_result and any(kw in user_input.lower() for kw in ['expand', ' more']):\n",
        "            current_result = system.ask_followup(user_input, current_result)\n",
        "        else:\n",
        "            current_result = system.process_request(user_input)\n",
        "\n",
        "        print(\"\\nHandover completed\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\nInterrupted by user\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Error: {str(e)}\")\n",
        "        print(\"Try again.\")\n",
        "\n",
        "print(\"\\nSystem is closing.\")"
      ],
      "metadata": {
        "id": "MxTiXgm0Yiwv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}